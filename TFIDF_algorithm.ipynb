{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello how ar you', 'I love you', 'done with you']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "words = [\"Hello how *ar3e && you  \",\n",
    "        \"I    love     you\",\n",
    "         \"1234 done with #you\"]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    words[i] = re.sub(r\"\\W\",\" \",words[i])\n",
    "    words[i] = re.sub(r\"\\d\",\" \",words[i])\n",
    "    words[i] = re.sub(r\"\\s+\",\" \",words[i])\n",
    "    words[i] = re.sub(r\"^\\s\",\"\",words[i])\n",
    "    words[i] = re.sub(r\"\\s$\",\"\",words[i])\n",
    "    words[i] = re.sub(r\"\\s+[a-z]\\s+\",\" \",words[i])\n",
    "    \n",
    "print(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re \n",
    "import heapq\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Native Americans lived in the Americas for thousands of years.\n",
    " English people in 1607 went to the place now called Jamestown, Virginia. \n",
    " Other European settlers went to the colonies, mostly from England and later Great Britain. France, Spain, and the Netherlands also colonized North America. \n",
    " In 1775, a war between the thirteen colonies and Britain began when the colonists were upset over changes in British policies.\n",
    " On July 4, 1776, rebel leaders made the United States Declaration of Independence. \n",
    " They won the Revolutionary War and started a new country. \n",
    " They signed the constitution in 1787 and the Bill of Rights in 1791. \n",
    " George Washington, who had led the war, became its first president. \n",
    " During the 19th century, the United States gained much more land in the West and began to become industrialized. \n",
    " In 1861, several states in the South left the United States to start a new country called the Confederate States of America.\n",
    " This caused the American Civil War. After the war, Immigration resumed. \n",
    " Some Americans became very rich in this Gilded Age and \n",
    " the country developed one of the largest economies in the world.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Libraries\n",
    "import nltk\n",
    "from nltk import PorterStemmer \n",
    "from nltk import WordNetLemmatizer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the paragraph in to sentencens in a list \n",
    "sentence = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the stemmer function \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# In the loop we are converting the sentence in to seperate words in a new list and implementing the stemmer \n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    new_words = [stemmer.stem(j) for j in words]\n",
    "    sentence[i] = ' '.join(new_words)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Lemmatizer function \n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    new_words = [lemmatizer.lemmatize(j) for j in words]\n",
    "    sentence[i] = ' '.join(new_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hitesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords \n",
    "\n",
    "# Importing the stopwords from nltk corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sentence Tokenization \n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Converting the sentence in to words using word tokenization and using stop words for english to remove unwanted words\n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    new_words = [ word for word in words if word not in stopwords.words('english') ]\n",
    "    sentence[i] = ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of Speech \n",
    "\n",
    "# converting the sentence in to words\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "new_tags = []\n",
    "\n",
    "\n",
    "for i in tagged_words:\n",
    "    new_tags.append(i[0] + \"_\" + i[1])\n",
    "    \n",
    "tagged_paragraph = ' '.join(new_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entities Recognition \n",
    "\n",
    "para = 'The Taj Mahal was build by Emporer Shah Jahan'\n",
    "# tokenizing the words\n",
    "\n",
    "words = nltk.word_tokenize(para)\n",
    "\n",
    "# tagging the words \n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "# Named entity will create a tree of words in to seperate category\n",
    "namedEntity = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "# use Draw() function to see the tree \n",
    "namedEntity.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building bag of words model \n",
    "\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    sentence[i]= sentence[i].lower()\n",
    "    sentence[i] = re.sub('\\W',' ',sentence[i])\n",
    "    sentence[i] = re.sub('\\s+',' ',sentence[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the count of words\n",
    "\n",
    "word2vec = {}\n",
    "\n",
    "for i in sentence:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for j in words:\n",
    "        if j not in word2vec.keys():\n",
    "            word2vec[j] = 1\n",
    "        else:\n",
    "            word2vec[j] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering only the top 50 word counts \n",
    "freq_words = heapq.nlargest(10,word2vec,key =word2vec.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createing the appearence of the words in a sentence as 1 else 0 \n",
    "x = []\n",
    "\n",
    "for i in sentence:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(i) :\n",
    "            vector.append(1)\n",
    "        else:\n",
    "             vector.append(0)\n",
    "                \n",
    "    x.append(vector)\n",
    "\n",
    "# converting the list of list X in to 2d array \n",
    "\n",
    "x = np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616],\n",
       " 'in': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616],\n",
       " 'and': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616],\n",
       " 'of': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616],\n",
       " 'war': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616],\n",
       " 'states': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616],\n",
       " 'to': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616],\n",
       " 'a': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616],\n",
       " 'united': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616],\n",
       " 'country': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.047619047619047616]}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the TF (Total Frequence)\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "sentence_appearence = []\n",
    "tf = {}\n",
    "for i in freq_words:\n",
    "    tf[i] = []\n",
    "    for j in sentence:\n",
    "        words_in_sent = nltk.word_tokenize(j)\n",
    "        temp = Counter(words_in_sent)\n",
    "        tf[i].append(temp[word]/len(words_in_sent))\n",
    "        \n",
    "    \n",
    "for i in sentence:\n",
    "    words_in_sent  = nltk.word_tokenize(i)\n",
    "    temp = Counter(words_in_sent)\n",
    "    word_appearence = []\n",
    "    for word in freq_words:\n",
    "        #word_appearence = []\n",
    "        word_appearence.append(temp[word])\n",
    "    \n",
    "    sentence_appearence.append(word_appearence)\n",
    "\n",
    "#sentence_appearence\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.5389965007326871,\n",
       " 'in': 0.8873031950009029,\n",
       " 'and': 0.8873031950009029,\n",
       " 'of': 1.0986122886681098,\n",
       " 'war': 1.0986122886681098,\n",
       " 'states': 1.4663370687934272,\n",
       " 'to': 1.252762968495368,\n",
       " 'a': 1.4663370687934272,\n",
       " 'united': 1.4663370687934272,\n",
       " 'country': 1.4663370687934272}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculation the IDF (Inverse Document Frequency)\n",
    "import math \n",
    "idf = {}\n",
    "for i in range(len(freq_words)):\n",
    "    count = 0\n",
    "    for j in range(len(sentence_appearence)):\n",
    "        if sentence_appearence[j][i] != 0:\n",
    "            count += 1\n",
    "            \n",
    "    idf[freq_words[i]] = np.log((len(sentence_appearence[i])/count)+1)\n",
    "\n",
    "\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 0\n",
    "tf_idf = []\n",
    "for i in idf:\n",
    "    tf_matrix = []\n",
    "    for j in tf[i]:\n",
    "        score = j * idf[i]\n",
    "        tf_matrix.append(score)\n",
    "    tf_idf.append(tf_matrix)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05389965, 0.        , 0.        , 0.        ,\n",
       "        0.02449984, 0.        , 0.        , 0.0256665 ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.08873032, 0.        , 0.        , 0.        ,\n",
       "        0.04033196, 0.        , 0.        , 0.04225253],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.08873032, 0.        , 0.        , 0.        ,\n",
       "        0.04033196, 0.        , 0.        , 0.04225253],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.10986123, 0.        , 0.        , 0.        ,\n",
       "        0.04993692, 0.        , 0.        , 0.05231487],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.10986123, 0.        , 0.        , 0.        ,\n",
       "        0.04993692, 0.        , 0.        , 0.05231487],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.14663371, 0.        , 0.        , 0.        ,\n",
       "        0.06665168, 0.        , 0.        , 0.06982557],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.1252763 , 0.        , 0.        , 0.        ,\n",
       "        0.05694377, 0.        , 0.        , 0.05965538],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.14663371, 0.        , 0.        , 0.        ,\n",
       "        0.06665168, 0.        , 0.        , 0.06982557],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.14663371, 0.        , 0.        , 0.        ,\n",
       "        0.06665168, 0.        , 0.        , 0.06982557],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.14663371, 0.        , 0.        , 0.        ,\n",
       "        0.06665168, 0.        , 0.        , 0.06982557]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix = np.asarray(tf_idf)\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global warming is a gradual increase in the overall temper\n"
     ]
    }
   ],
   "source": [
    "#Implementation of character N-gram modeling \n",
    "import random \n",
    "\n",
    "definition = \"Global warming is a gradual increase in the overall temperature of the earth's atmosphere generally attributed to the greenhouse effect caused by increased levels of carbon dioxide,chlorofluorocarbons, and other pollutants\"\n",
    "\n",
    "# AS we have mentioned n '3' it is a try gram model \n",
    "n = 8\n",
    "\n",
    "ngrams = {}\n",
    "\n",
    "for i in range(0,len(definition)-n):\n",
    "    if  definition[i:n+i] not in ngrams.keys():\n",
    "        ngrams[definition[i:n+i]] = []\n",
    "    ngrams[definition[i:n+i]].append(definition[n+i])\n",
    "\n",
    "    \n",
    "    \n",
    "# Testing our ngram model, predicting the summary of the entire definition \n",
    "\n",
    "currentgram = definition[0:n]\n",
    "result = currentgram \n",
    "\n",
    "for i in range(50):\n",
    "    if currentgram not in ngrams.keys():\n",
    "        break\n",
    "    possibility = ngrams[currentgram]\n",
    "    nextitem = possibility[random.randrange(len(possibility))]\n",
    "    result += nextitem \n",
    "    currentgram = result[len(result)-n : len(result)]\n",
    " \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global warming is a\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "\n",
    "definition = \"Global warming is a gradual increase in the overall temperature of the earth's atmosphere generally attributed to the greenhouse effect caused by increased levels of carbon dioxide,chlorofluorocarbons, and other pollutants\"\n",
    "\n",
    "\n",
    "ngrams = {}\n",
    "\n",
    "words =nltk.word_tokenize(definition)\n",
    "\n",
    "for i in range(len(words)-n):\n",
    "    gram = ' '.join(words[i:i+n])\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram] = []\n",
    "    ngrams[gram].append(words[i+n])\n",
    "    \n",
    "    \n",
    "currentgram = ' '.join(words[0:n])\n",
    "result = currentgram\n",
    "for i in range(80):\n",
    "    if result not in ngrams.keys():\n",
    "        break\n",
    "    possibility = ngrams[currentgram]\n",
    "    nextitem = possibility[random.randrange(len(possibility))]\n",
    "    result += ' '+nextitem\n",
    "    rwords= nltk.word_tokenize(result)\n",
    "    currentgram = ' '.join(rwords[len(rwords)-n:len(rwords)])\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent semantic Analysis (It is used to find the category of document to which it belongs based o the content in the document)\n",
    "\n",
    "# It is done using the SVD (Singular Value Decomposition)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "dataset = ['The amount of pollution increased day by day',\n",
    "          'The concert was just great',\n",
    "          'I love to see gordan ramsay cooking',\n",
    "          'Google is introduction a new technology',\n",
    "          'AI robots are great example of technology today',\n",
    "          'All of us where singning in concert',\n",
    "          'we have lunched campaign to stop pollution and global warming']\n",
    "\n",
    "dataset = [i.lower() for i in dataset]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "\n",
    "lsa = TruncatedSVD(n_components = 4, n_iter = 200)\n",
    "lsa.fit(X)\n",
    "\n",
    "concept_words = {}\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    components = zip(terms,comp)\n",
    "    sortedterms = sorted(components, key = lambda x:x[1], reverse = True)\n",
    "    sortedterms = sortedterms[:10]\n",
    "    concept_words['concept'+str(i)] = sortedterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "concept0:\n",
      "1.1228661615471252\n",
      "1.4549121840458572\n",
      "0\n",
      "0.20178823791952946\n",
      "1.1911124219041576\n",
      "0.6515262592149066\n",
      "0\n",
      "\n",
      "concept1:\n",
      "0.24559673708166066\n",
      "0\n",
      "1.553813139719812\n",
      "0\n",
      "0\n",
      "0\n",
      "1.2595398249654894\n",
      "\n",
      "concept2:\n",
      "0\n",
      "0\n",
      "0\n",
      "1.744038223712797\n",
      "1.0916695895403328\n",
      "0\n",
      "0\n",
      "\n",
      "concept3:\n",
      "0\n",
      "0.2001917668052002\n",
      "1.2936837572578126\n",
      "0\n",
      "0\n",
      "0.8800798554300745\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for key in concept_words.keys():\n",
    "    sentence_scores =[]\n",
    "    for sentence in dataset:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        score = 0 \n",
    "        for word in words:\n",
    "            for word_score in concept_words[key]:\n",
    "                if word == word_score[0]:\n",
    "                    score += word_score[1]\n",
    "        sentence_scores.append(score)\n",
    "    print(\"\\n\"+key+\":\")\n",
    "    for sentence_score in sentence_scores:\n",
    "        print(sentence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evil', 'evilness', 'bad', 'badness', 'ill']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding synonyms and antonyms of a word \n",
    "\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for i in wordnet.synsets('good'):\n",
    "    for j  in i.lemmas():\n",
    "        if j.name() not in synonyms:\n",
    "            synonyms.append(j.name())\n",
    "        for k in j.antonyms():\n",
    "            if k.name() not in antonyms:\n",
    "                antonyms.append(k.name())\n",
    "        \n",
    "antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I was not_happy with the team 's performance\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Negation Tracking \n",
    "\n",
    "import nltk \n",
    "\n",
    "sentence = \"I was not happy with the team's performance\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "new_words = []\n",
    "\n",
    "temp_word = \"\"\n",
    "\n",
    "for word in words:\n",
    "    if word == 'not':\n",
    "        temp_word = 'not_'\n",
    "    elif temp_word == 'not_':\n",
    "        word = temp_word + word\n",
    "        temp_word = ''\n",
    "    if word != 'not':\n",
    "        new_words.append(word)\n",
    "        \n",
    "sentence = ' '.join(new_words)\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I was unhappy with the team 's performance\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negation using Antonyms\n",
    "\n",
    "import nltk \n",
    "\n",
    "sentence = \"I was not happy with the team's performance\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "new_words = []\n",
    "\n",
    "temp_word = \"\"\n",
    "\n",
    "\n",
    "antonyms = []\n",
    "for word in words:\n",
    "    if word == 'not':\n",
    "        temp_word = 'not_'\n",
    "    elif temp_word == 'not_':\n",
    "        for i in wordnet.synsets(word):\n",
    "            for j  in i.lemmas():\n",
    "                 for k in j.antonyms():\n",
    "                    if k.name() not in antonyms:\n",
    "                        antonyms.append(k.name())\n",
    "        if len(antonyms) >= 1:\n",
    "            word = antonyms[0]\n",
    "        else:    \n",
    "            word = temp_word + word\n",
    "        temp_word = ''\n",
    "    if word != 'not':\n",
    "        new_words.append(word)\n",
    "        \n",
    "sentence = ' '.join(new_words)\n",
    "\n",
    "sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
